{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In binary classification, where there are two classes (0 and 1), the cross-entropy loss for a single sample can be calculated as follows:\n",
    "\n",
    "Let y be the true label (0 or 1)\n",
    "Let p be the predicted probability of the positive class (class 1)\n",
    "Then, the cross-entropy loss for that sample is given by: -(y * log(p) + (1-y) * log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate number of samples\n",
    "    m = y_true.shape[0]\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, entropy is a measure of the impurity or uncertainty of a dataset. It is a concept that is commonly used in decision tree algorithms and other models that involve binary or multi-class classification.\n",
    "\n",
    "In binary classification, entropy is used to calculate the homogeneity of a set of data points that belong to two classes. The entropy is calculated as:\n",
    "\n",
    "H = -p * log2(p) - (1 - p) * log2(1 - p)\n",
    "\n",
    "where p is the proportion of data points that belong to one of the two classes.\n",
    "\n",
    "In multi-class classification, entropy is calculated similarly, but is based on the proportions of data points that belong to each class. The entropy formula for multi-class classification is:\n",
    "\n",
    "H = -sum(p_i * log2(p_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * math.log2(p) - (1 - p) * math.log2(1 - p)\n",
    "\n",
    "# Example usage\n",
    "p = 0.7 # Proportion of positive data points\n",
    "H = entropy(p)\n",
    "print(f\"Entropy: {H}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.3602012209808308\n"
     ]
    }
   ],
   "source": [
    "def entropy(probabilities):\n",
    "    probabilities = np.array(probabilities)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "p = 0.7 # Proportion of positive data points\n",
    "H = entropy(p) \n",
    "print(f\"Entropy: {H}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is a measure of the amount of information that one random variable provides about another random variable. In machine learning, mutual information is often used to select features that are most relevant to a particular task.\n",
    "\n",
    "To calculate mutual information between two variables, we need to calculate the entropy of each variable and the conditional entropy of one variable given the other variable. We can do this using the entropy function that we defined earlier as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Calculate joint probabilities\n",
    "    joint_prob = np.histogram2d(x, y)[0] / float(x.size)\n",
    "    \n",
    "    # Calculate marginal probabilities\n",
    "    x_prob = np.histogram(x)[0] / float(x.size)\n",
    "    y_prob = np.histogram(y)[0] / float(y.size)\n",
    "    \n",
    "    # Calculate entropies\n",
    "    h_x = -np.sum(x_prob * np.log2(x_prob))\n",
    "    h_y = -np.sum(y_prob * np.log2(y_prob))\n",
    "    h_xy = -np.sum(joint_prob * np.log2(joint_prob))\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi = h_x + h_y - h_xy\n",
    "    \n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional entropy is a measure of the amount of uncertainty in a random variable given the value of another random variable. In other words, it measures the amount of uncertainty in a variable after we have observed the value of another variable.\n",
    "\n",
    "To calculate conditional entropy in Python, we can use the entropy function that we defined earlier to calculate the entropy of each variable and the joint entropy of the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Calculate joint probabilities\n",
    "    joint_prob = np.histogram2d(x, y)[0] / float(x.size)\n",
    "    \n",
    "    # Calculate marginal probabilities\n",
    "    y_prob = np.histogram(y)[0] / float(y.size)\n",
    "    \n",
    "    # Calculate conditional probabilities\n",
    "    cond_prob = joint_prob / y_prob\n",
    "    \n",
    "    # Calculate conditional entropy\n",
    "    cond_entropy = -np.sum(cond_prob * np.log2(cond_prob))\n",
    "    \n",
    "    return cond_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence, is a measure of the difference between two probability distributions. It measures how much information is lost when one distribution is used to approximate another distribution. KL divergence is commonly used in machine learning to compare a predicted probability distribution with a true probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    p = np.clip(p, 1e-8, None)\n",
    "    q = np.clip(q, 1e-8, None)\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    kl_div = np.sum(p * np.log2(p / q))\n",
    "    \n",
    "    return kl_div\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
